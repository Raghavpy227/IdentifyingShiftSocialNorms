{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86512a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:16:27.341543Z",
     "iopub.status.busy": "2023-08-12T13:16:27.341196Z",
     "iopub.status.idle": "2023-08-12T13:16:54.230850Z",
     "shell.execute_reply": "2023-08-12T13:16:54.229866Z",
     "shell.execute_reply.started": "2023-08-12T13:16:27.341514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from openprompt.data_utils import InputExample\n",
    "import torch\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openprompt.config import get_config, save_config_to_yaml\n",
    "from tqdm.contrib import tenumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a76a9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:16:54.233114Z",
     "iopub.status.busy": "2023-08-12T13:16:54.232767Z",
     "iopub.status.idle": "2023-08-12T13:17:00.364543Z",
     "shell.execute_reply": "2023-08-12T13:17:00.363489Z",
     "shell.execute_reply.started": "2023-08-12T13:16:54.233087Z"
    }
   },
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\py22715\\OneDrive - University of Bristol\\Documents\\Python Scripts\"\n",
    "df=pd.read_csv(os.path.join(path,\"golden_set2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a274be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:00.366980Z",
     "iopub.status.busy": "2023-08-12T13:17:00.366236Z",
     "iopub.status.idle": "2023-08-12T13:17:00.394685Z",
     "shell.execute_reply": "2023-08-12T13:17:00.393319Z",
     "shell.execute_reply.started": "2023-08-12T13:17:00.366942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Homophobia': 0, 'Incivility': 1, 'No Violations': 2, 'harrassment': 3, 'racial_slur': 4}\n"
     ]
    }
   ],
   "source": [
    "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "    \"racial_slur\",\n",
    "    \"self_harm\",\n",
    "    \"Homophobia\",\n",
    "    \"Incivility\",\n",
    "    \"harrassment\",\n",
    "    \"No Violations\"\n",
    "]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df[\"label\"]=encoder.fit_transform(df[\"violation\"])\n",
    "\n",
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "print(le_name_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e1c004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:00.398349Z",
     "iopub.status.busy": "2023-08-12T13:17:00.398049Z",
     "iopub.status.idle": "2023-08-12T13:17:09.617251Z",
     "shell.execute_reply": "2023-08-12T13:17:09.616245Z",
     "shell.execute_reply.started": "2023-08-12T13:17:00.398324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.lm_bff_trainer import LMBFFClassificationRunner,ClassificationRunner\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdebfaf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:09.619092Z",
     "iopub.status.busy": "2023-08-12T13:17:09.618641Z",
     "iopub.status.idle": "2023-08-12T13:17:09.626259Z",
     "shell.execute_reply": "2023-08-12T13:17:09.625089Z",
     "shell.execute_reply.started": "2023-08-12T13:17:09.619056Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "\n",
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} was the context for {\"placeholder\":\"text_b\"}, Which violation is it? {\"mask\"}' ,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "bad_words = {\n",
    "    \"racial_slur\": [\"nigga\",\"nigger\",\"uncle tom\",\"negro\",\"niggerhead\",\"house slave\",\"monkeyboy\"],\n",
    "    \"self_harm\": [\"kill yourself\", \"commit suicide\"],\n",
    "    \"Homophobia\":[\"lesbo\",\"faggot\",\"fag\",'cocksucker'],\n",
    "    \"Incivility\":[\"dickhead\",\"twat\",\"cunt\",\"whore\",\"retard\",\"bitch\",\"asshole\",\"dimwit\",\"bullshit\",\"fuck u\",\"fuck you\",\"motherfuck\"],\n",
    "    \"harrassment\":[\"penis\",\"pussy\",\"dick\",\"tits\",\"moron\",\"suck dick\",\"sexy bitch\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4a00ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:09.628131Z",
     "iopub.status.busy": "2023-08-12T13:17:09.627774Z",
     "iopub.status.idle": "2023-08-12T13:17:27.294278Z",
     "shell.execute_reply": "2023-08-12T13:17:27.293297Z",
     "shell.execute_reply.started": "2023-08-12T13:17:09.628097Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_set=set()\n",
    "for k,v in bad_words.items():\n",
    "    for i in bad_words[k]: \n",
    "        bad_set.add(i)\n",
    "all_set=set()\n",
    "for i in df[\"context\"]:\n",
    "   for k in i.split(' '):\n",
    "       all_set.add(k.lower())\n",
    "    \n",
    "no_vio = all_set.difference(bad_set)  \n",
    "no_vio=list(no_vio)\n",
    "no_vio=no_vio[0:len(no_vio):5000]\n",
    "\n",
    "    \n",
    " \n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"racial_slur\": [\"nigga\",\"nigger\",\"uncle tom\",\"negro\",\"niggerhead\",\"house slave\",\"monkeyboy\"],\n",
    "        \"self_harm\": [\"kill yourself\", \"commit suicide\"],\n",
    "        \"Homophobia\":[\"lesbo\",\"faggot\",\"fag\",'cocksucker'],\n",
    "        \"Incivility\":[\"dickhead\",\"twat\",\"cunt\",\"whore\",\"retard\",\"bitch\",\"asshole\",\"dimwit\",\"bullshit\",\"fuck u\",\"fuck you\",\"motherfuck\"],\n",
    "        \"harrassment\":[\"penis\",\"pussy\",\"dick\",\"tits\",\"moron\",\"suck dick\",\"sexy bitch\"],\n",
    "        \"No Violations\": no_vio\n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6abd5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:27.297973Z",
     "iopub.status.busy": "2023-08-12T13:17:27.297677Z",
     "iopub.status.idle": "2023-08-12T13:17:27.302983Z",
     "shell.execute_reply": "2023-08-12T13:17:27.301899Z",
     "shell.execute_reply.started": "2023-08-12T13:17:27.297947Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    "    freeze_plm=False,\n",
    "    plm_eval_mode=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17db5903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:17:27.304744Z",
     "iopub.status.busy": "2023-08-12T13:17:27.304200Z",
     "iopub.status.idle": "2023-08-12T13:44:44.548118Z",
     "shell.execute_reply": "2023-08-12T13:44:44.547101Z",
     "shell.execute_reply.started": "2023-08-12T13:17:27.304708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset\n",
      "Creating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 173it [00:03, 55.64it/s]\n",
      "tokenizing: 44it [00:00, 50.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndata_loader_test = PromptDataLoader(\\n    dataset = data_test,\\n    tokenizer = tokenizer,\\n    template = promptTemplate,\\n    tokenizer_wrapper_class=WrapperClass,\\n    shuffle=True\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train , data_val = train_test_split(df,train_size=0.80,random_state=2018)\n",
    "#data_train,data_val = train_test_split(data_train_val,train_size=0.25,random_state=2018,stratify=data_train_val['label'])\n",
    "\n",
    "def InputExampleConverter(df): \n",
    "    dataset=[]\n",
    "    count=0\n",
    "    violation_sentence = df[\"sentence\"]\n",
    "    label=df[\"label\"]\n",
    "    context=df[\"context\"]\n",
    "    print(\"Creating Dataset\")     \n",
    "    for i in violation_sentence: \n",
    "        dataset.append(InputExample(guid=count,text_a=context[count],text_b=i,label=label[count]))\n",
    "        count=count+1\n",
    "    return(dataset)\n",
    "data_train=InputExampleConverter(data_train.reset_index())\n",
    "data_val=InputExampleConverter(data_val.reset_index())\n",
    "#data_test=InputExampleConverter(data_test.reset_index())    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "data_loader_train = PromptDataLoader(\n",
    "    dataset = data_train,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "data_loader_val = PromptDataLoader(\n",
    "    dataset = data_val,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    shuffle=True\n",
    ")\n",
    "'''\n",
    "data_loader_test = PromptDataLoader(\n",
    "    dataset = data_test,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    shuffle=True\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8def47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:44:44.550443Z",
     "iopub.status.busy": "2023-08-12T13:44:44.549792Z",
     "iopub.status.idle": "2023-08-12T13:44:49.890393Z",
     "shell.execute_reply": "2023-08-12T13:44:49.889394Z",
     "shell.execute_reply.started": "2023-08-12T13:44:44.550403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    promptModel=  promptModel.cuda()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Now the training is standard\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(promptModel.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a03c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T13:44:49.894630Z",
     "iopub.status.busy": "2023-08-12T13:44:49.893820Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ec25343f84fea9aeed9abd9426cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 1.5301582217216492\n",
      "getting val score after epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e0061dad8c47e78406ef6e7e3badd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.11      0.20         9\n",
      "           2       0.70      1.00      0.82        30\n",
      "           3       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.70        44\n",
      "   macro avg       0.57      0.37      0.34        44\n",
      "weighted avg       0.68      0.70      0.60        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4800be8d5c2741bd90fc75453809e095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average loss: 0.5804618149995804\n",
      "getting val score after epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f422e320ca43f190a0e61cd85ef5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6363636363636364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.78      0.48         9\n",
      "           2       0.88      0.70      0.78        30\n",
      "           3       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.64        44\n",
      "   macro avg       0.41      0.49      0.42        44\n",
      "weighted avg       0.67      0.64      0.63        44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc9fef2fbf0442ba678b0597f883293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, average loss: 0.5237976014614105\n",
      "getting val score after epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d5cf22936b4842827a70750ac9f0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.71      0.97      0.82        30\n",
      "           3       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.70        44\n",
      "   macro avg       0.57      0.46      0.46        44\n",
      "weighted avg       0.60      0.70      0.62        44\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1d333fad6f4a25a074bc17d8e0afee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, average loss: 0.045423186384141445\n",
      "getting val score after epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358260c0fc50447294e3aaf9dbb274ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.44      0.42         9\n",
      "           2       0.86      0.83      0.85        30\n",
      "           3       0.40      0.40      0.40         5\n",
      "\n",
      "    accuracy                           0.70        44\n",
      "   macro avg       0.55      0.56      0.56        44\n",
      "weighted avg       0.72      0.70      0.71        44\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18ab05d9d5842d9b6e5e2014bd24697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, average loss: 0.09154116152785718\n",
      "getting val score after epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d690a97d1ac4272aad55ef8eb94a0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045454545454546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.33      0.38         9\n",
      "           2       0.89      0.80      0.84        30\n",
      "           3       0.40      0.80      0.53         5\n",
      "\n",
      "    accuracy                           0.70        44\n",
      "   macro avg       0.57      0.64      0.58        44\n",
      "weighted avg       0.74      0.70      0.71        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    tot_loss = 0\n",
    "    prev_score=0\n",
    "    val_score=[]\n",
    "    test_score=[]\n",
    "    for step, inputs in tenumerate(data_loader_train):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.to(device) \n",
    "        #print(step)   \n",
    "        logits = promptModel(inputs)\n",
    "        logits=logits.to(device)\n",
    "        labels = inputs['label']\n",
    "        labels=labels.to(device)\n",
    "        loss = loss_func(logits,labels.type(torch.LongTensor).to(device))\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %5000 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "            \n",
    "    try: \n",
    "            torch.save({\n",
    "\n",
    "            'epoch': epoch,\n",
    "\n",
    "            'model_state_dict': promptModel.state_dict(),\n",
    "\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "            'loss': loss,\n",
    "                    \n",
    "            'model':promptModel       \n",
    "\n",
    "            },os.path.join(path,\"violation_type_context_model_chkpt.pt\"))\n",
    "                \n",
    "    except:\n",
    "            continue      \n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    print(\"getting val score after epoch \"+ str(epoch))\n",
    "    for step, inputs in tenumerate(data_loader_val):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = promptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "    print(f1_score(alllabels,allpreds,average=\"micro\" ))\n",
    "    print(classification_report(alllabels,allpreds))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4fddee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-11T21:50:20.522844Z",
     "iopub.status.idle": "2023-08-11T21:50:20.523340Z",
     "shell.execute_reply": "2023-08-11T21:50:20.523107Z",
     "shell.execute_reply.started": "2023-08-11T21:50:20.523084Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(promptModel,os.path.join(path,\"violation_detector.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19094696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
