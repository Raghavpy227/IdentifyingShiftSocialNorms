{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694c838c",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "The aim of this script is to generate baseline models which use pretarined models like BERT , T5 and gpt2 and compare the performance of it vs a trained openprompt model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb60abe",
   "metadata": {},
   "source": [
    "## Imports and Datapreprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e29936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping preprocessor as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor==0.6.0 in c:\\users\\py22715\\.conda\\envs\\v38\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import warnings\n",
    "import redditcleaner\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import redditcleaner\n",
    "!pip uninstall preprocessor\n",
    "!pip install tweet-preprocessor==0.6.0\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.contrib import tenumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e1832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\py22715\\OneDrive - University of Bristol\\Documents\\Python Scripts\"\n",
    "df=pd.read_csv(os.path.join(path,\"golden_set2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Incivility</td>\n",
       "      <td>\"Hey douchebag, use the NFL as your example o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  subreddit         created_date  \\\n",
       "0  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "1  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "2  819lh     sports  2009-03-01 21:38:40   \n",
       "3  819lh     sports  2009-03-01 21:38:40   \n",
       "4  819lh     sports  2009-03-01 21:38:40   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "1  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "2  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "3  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "4  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "\n",
       "       violation                                           sentence  \n",
       "0  No Violations   \"It's worse to be passive-aggressive. It gets...  \n",
       "1  No Violations   \"It's worse to be passive-aggressive. It gets...  \n",
       "2     Incivility   \"Hey douchebag, use the NFL as your example o...  \n",
       "3    harrassment   \"This coming from a sport that resolves it pr...  \n",
       "4    harrassment   \"This coming from a sport that resolves it pr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fdb4e",
   "metadata": {},
   "source": [
    "### Defining functions for cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2432ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def reddit_clean(x):\n",
    "            \n",
    "            return tokenizer.tokenize(x)\n",
    "def reddit_batch_clean(x):\n",
    "            return [tokenizer.tokenize(redditcleaner.clean(e)) for e in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e0b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sentences=df['sentence']\n",
    "#df.drop(['sentence'],axis=1,inplace= True)\n",
    "final_sentences=[]\n",
    "for i in sentences:\n",
    "    final_sentences.append(reddit_clean(i))    \n",
    "df['final_sentences']=final_sentences    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350c6b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "      <th>final_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "      <td>[\", It, ', s, worse, to, be, passive, -, aggre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "      <td>[\", It, ', s, worse, to, be, passive, -, aggre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Incivility</td>\n",
       "      <td>\"Hey douchebag, use the NFL as your example o...</td>\n",
       "      <td>[\", Hey, do, ##uche, ##bag, ,, use, the, NFL, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "      <td>[\", This, coming, from, a, sport, that, resolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "      <td>[\", This, coming, from, a, sport, that, resolv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  subreddit         created_date  \\\n",
       "0  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "1  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "2  819lh     sports  2009-03-01 21:38:40   \n",
       "3  819lh     sports  2009-03-01 21:38:40   \n",
       "4  819lh     sports  2009-03-01 21:38:40   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "1  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "2  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "3  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "4  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "\n",
       "       violation                                           sentence  \\\n",
       "0  No Violations   \"It's worse to be passive-aggressive. It gets...   \n",
       "1  No Violations   \"It's worse to be passive-aggressive. It gets...   \n",
       "2     Incivility   \"Hey douchebag, use the NFL as your example o...   \n",
       "3    harrassment   \"This coming from a sport that resolves it pr...   \n",
       "4    harrassment   \"This coming from a sport that resolves it pr...   \n",
       "\n",
       "                                     final_sentences  \n",
       "0  [\", It, ', s, worse, to, be, passive, -, aggre...  \n",
       "1  [\", It, ', s, worse, to, be, passive, -, aggre...  \n",
       "2  [\", Hey, do, ##uche, ##bag, ,, use, the, NFL, ...  \n",
       "3  [\", This, coming, from, a, sport, that, resolv...  \n",
       "4  [\", This, coming, from, a, sport, that, resolv...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_sentences']=final_sentences  \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96d3fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "      <th>final_sentences</th>\n",
       "      <th>final_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "      <td>[\", It, ', s, worse, to, be, passive, -, aggre...</td>\n",
       "      <td>[[, ', \", I, wasn, \\, ', t, particularly, taug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813yb</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>2009-03-01 00:43:33</td>\n",
       "      <td>[' \"I wasn\\'t particularly taught that being p...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"It's worse to be passive-aggressive. It gets...</td>\n",
       "      <td>[\", It, ', s, worse, to, be, passive, -, aggre...</td>\n",
       "      <td>[[, ', \", I, wasn, \\, ', t, particularly, taug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Incivility</td>\n",
       "      <td>\"Hey douchebag, use the NFL as your example o...</td>\n",
       "      <td>[\", Hey, do, ##uche, ##bag, ,, use, the, NFL, ...</td>\n",
       "      <td>[[, ', \", Yeah, ,, Don, Cherry, is, going, ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "      <td>[\", This, coming, from, a, sport, that, resolv...</td>\n",
       "      <td>[[, ', \", Yeah, ,, Don, Cherry, is, going, ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819lh</td>\n",
       "      <td>sports</td>\n",
       "      <td>2009-03-01 21:38:40</td>\n",
       "      <td>[' \"Yeah, Don Cherry is going over the line he...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"This coming from a sport that resolves it pr...</td>\n",
       "      <td>[\", This, coming, from, a, sport, that, resolv...</td>\n",
       "      <td>[[, ', \", Yeah, ,, Don, Cherry, is, going, ove...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  subreddit         created_date  \\\n",
       "0  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "1  813yb  AskReddit  2009-03-01 00:43:33   \n",
       "2  819lh     sports  2009-03-01 21:38:40   \n",
       "3  819lh     sports  2009-03-01 21:38:40   \n",
       "4  819lh     sports  2009-03-01 21:38:40   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "1  [' \"I wasn\\'t particularly taught that being p...           No   \n",
       "2  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "3  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "4  [' \"Yeah, Don Cherry is going over the line he...          Yes   \n",
       "\n",
       "       violation                                           sentence  \\\n",
       "0  No Violations   \"It's worse to be passive-aggressive. It gets...   \n",
       "1  No Violations   \"It's worse to be passive-aggressive. It gets...   \n",
       "2     Incivility   \"Hey douchebag, use the NFL as your example o...   \n",
       "3    harrassment   \"This coming from a sport that resolves it pr...   \n",
       "4    harrassment   \"This coming from a sport that resolves it pr...   \n",
       "\n",
       "                                     final_sentences  \\\n",
       "0  [\", It, ', s, worse, to, be, passive, -, aggre...   \n",
       "1  [\", It, ', s, worse, to, be, passive, -, aggre...   \n",
       "2  [\", Hey, do, ##uche, ##bag, ,, use, the, NFL, ...   \n",
       "3  [\", This, coming, from, a, sport, that, resolv...   \n",
       "4  [\", This, coming, from, a, sport, that, resolv...   \n",
       "\n",
       "                                       final_context  \n",
       "0  [[, ', \", I, wasn, \\, ', t, particularly, taug...  \n",
       "1  [[, ', \", I, wasn, \\, ', t, particularly, taug...  \n",
       "2  [[, ', \", Yeah, ,, Don, Cherry, is, going, ove...  \n",
       "3  [[, ', \", Yeah, ,, Don, Cherry, is, going, ove...  \n",
       "4  [[, ', \", Yeah, ,, Don, Cherry, is, going, ove...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context=df['context']\n",
    "final_context=[]\n",
    "for i in context:\n",
    "    final_context.append(reddit_clean(i))\n",
    "df['final_context']=final_context  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c316fa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"label\"]=encoder.fit_transform(df[\"violation\"])\n",
    "df['sentence']=df['context']+df['sentence']\n",
    "\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(np.array(df['sentence']).flatten().tolist(), df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    train_size=0.8 \n",
    "                                                                    )\n",
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0bff00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "   list(train_text),\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "   list(val_text),\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd42f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "582d7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y = torch.tensor(list(val_labels))\n",
    "train_y = torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687ce344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-cased\",return_dict=False)\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867bb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert,label_map):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,len(label_map))\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e5cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map=le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "model = BERT_Arch(bert,label_map)\n",
    "\n",
    "# push the model to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d591005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8120770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    total_labels =[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in tenumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 5000 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad()        \n",
    "        preds = model(sent_id,mask)\n",
    "        loss = loss_func(preds, labels)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.tolist()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    f1 = f1_score(total_labels, total_preds, average='micro')\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f15c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step,batch in tenumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 5000 == 0 and not step == 0:\n",
    "\n",
    "          # Calculate elapsed time in minutes.\n",
    "          #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "          # Report progress.\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_func(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            total_preds+=list(preds)\n",
    "            total_labels+=labels.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    f1 = f1_score(total_labels, total_preds, average='micro')\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18453f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7222463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce1980a6ebf4ac2b9e254a9c0e27240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2505881147233049\n",
      "0.6184971098265896\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339a374c438c45909d9ee1e5e0cf8ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2505881147233049\n",
      "0.6184971098265896\n",
      "\n",
      "Training Loss: 1.251\n",
      "Validation Loss: 0.975\n",
      "\n",
      "Training F1: 0.618\n",
      "Validation F1: 0.682\n",
      "\n",
      " Epoch 2 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83db62ec1714fe8bb7975a0dc96556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.1694718493805456\n",
      "0.6184971098265896\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6424c4191f074035a8db27760b402c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.1694718493805456\n",
      "0.6184971098265896\n",
      "\n",
      "Training Loss: 1.169\n",
      "Validation Loss: 1.068\n",
      "\n",
      "Training F1: 0.618\n",
      "Validation F1: 0.682\n",
      "\n",
      " Epoch 3 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4018f6db626945a19f83fc84ad6aac15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.4780068923907645\n",
      "0.6184971098265896\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86c0ea2a3046beb540ac85ae61124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.4780068923907645\n",
      "0.6184971098265896\n",
      "\n",
      "Training Loss: 1.478\n",
      "Validation Loss: 1.307\n",
      "\n",
      "Training F1: 0.618\n",
      "Validation F1: 0.682\n",
      "\n",
      " Epoch 4 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515dffca8bd34e61a612a842a8557464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.687546156465232\n",
      "0.6184971098265896\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32867cc43bd429897dd158526a0eff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.687546156465232\n",
      "0.6184971098265896\n",
      "\n",
      "Training Loss: 1.688\n",
      "Validation Loss: 1.370\n",
      "\n",
      "Training F1: 0.618\n",
      "Validation F1: 0.682\n",
      "\n",
      " Epoch 5 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1297796937b4ea6bffd2a2936838732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.725835507660243\n",
      "0.6184971098265896\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46094b8d8deb41358a63944b527538a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.725835507660243\n",
      "0.6184971098265896\n",
      "\n",
      "Training Loss: 1.726\n",
      "Validation Loss: 1.388\n",
      "\n",
      "Training F1: 0.618\n",
      "Validation F1: 0.682\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, f1_train = train()\n",
    "    print(\"Training loss and F1\")\n",
    "    print(train_loss)\n",
    "    print(f1_train)\n",
    "    #evaluate model\n",
    "    valid_loss, f1_valid = evaluate()\n",
    "    print(\"Training loss and F1\")\n",
    "    print(train_loss)\n",
    "    print(f1_train)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        file_name = 'topic_saved_weights.pt'\n",
    "        #save_checkpoint(file_name, epoch, model, optimizer, label_map, id2label)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "    print(f'\\nTraining F1: {f1_train:.3f}')\n",
    "    print(f'Validation F1: {f1_valid:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c9866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
