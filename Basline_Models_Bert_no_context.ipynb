{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694c838c",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "The aim of this script is to generate baseline models which use pretarined models like BERT and compare the performance of it vs a trained openprompt model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb60abe",
   "metadata": {},
   "source": [
    "## Imports and Datapreprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e29936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Skipping preprocessor as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor==0.6.0 in c:\\users\\py22715\\.conda\\envs\\v38\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import warnings\n",
    "import redditcleaner\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import redditcleaner\n",
    "!pip uninstall preprocessor\n",
    "!pip install tweet-preprocessor==0.6.0\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.contrib import tenumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e1832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\py22715\\OneDrive - University of Bristol\\Documents\\Python Scripts\"\n",
    "df=pd.read_csv(os.path.join(path,\"RS_2012-03_violations2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"OP, you're a complete moron and an embarrass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Irrational hate of something they don't unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Let me clarify this. I don't believe that an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"DAmnnit, woke up to find this xD I've preord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"I would like to try TERA without waiting so ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     subreddit         created_date  \\\n",
       "0  qdole  Conservative  2012-03-01 23:59:45   \n",
       "1  qdole  Conservative  2012-03-01 23:59:45   \n",
       "2  qdole  Conservative  2012-03-01 23:59:45   \n",
       "3  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "4  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  ['[\"The hypocrisy is interesting to watch, tha...          Yes   \n",
       "1  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "2  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "3  ['[\"well i would say i want one because the ga...           No   \n",
       "4  ['[\"well i would say i want one because the ga...           No   \n",
       "\n",
       "       violation                                           sentence  \n",
       "0    harrassment   \"OP, you're a complete moron and an embarrass...  \n",
       "1  No Violations   \"Irrational hate of something they don't unde...  \n",
       "2  No Violations   \"Let me clarify this. I don't believe that an...  \n",
       "3  No Violations   \"DAmnnit, woke up to find this xD I've preord...  \n",
       "4  No Violations   \"I would like to try TERA without waiting so ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fdb4e",
   "metadata": {},
   "source": [
    "### Defining functions for cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2432ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def reddit_clean(x):\n",
    "            \n",
    "            return tokenizer.tokenize(x)\n",
    "def reddit_batch_clean(x):\n",
    "            return [tokenizer.tokenize(redditcleaner.clean(e)) for e in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e0b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sentences=df['sentence']\n",
    "#df.drop(['sentence'],axis=1,inplace= True)\n",
    "final_sentences=[]\n",
    "for i in sentences:\n",
    "    final_sentences.append(reddit_clean(i))    \n",
    "df['final_sentences']=final_sentences    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350c6b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "      <th>final_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"OP, you're a complete moron and an embarrass...</td>\n",
       "      <td>[\", op, ,, you, ', re, a, complete, mor, ##on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Irrational hate of something they don't unde...</td>\n",
       "      <td>[\", irrational, hate, of, something, they, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Let me clarify this. I don't believe that an...</td>\n",
       "      <td>[\", let, me, clarify, this, ., i, don, ', t, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"DAmnnit, woke up to find this xD I've preord...</td>\n",
       "      <td>[\", damn, ##ni, ##t, ,, woke, up, to, find, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"I would like to try TERA without waiting so ...</td>\n",
       "      <td>[\", i, would, like, to, try, ter, ##a, without...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     subreddit         created_date  \\\n",
       "0  qdole  Conservative  2012-03-01 23:59:45   \n",
       "1  qdole  Conservative  2012-03-01 23:59:45   \n",
       "2  qdole  Conservative  2012-03-01 23:59:45   \n",
       "3  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "4  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  ['[\"The hypocrisy is interesting to watch, tha...          Yes   \n",
       "1  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "2  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "3  ['[\"well i would say i want one because the ga...           No   \n",
       "4  ['[\"well i would say i want one because the ga...           No   \n",
       "\n",
       "       violation                                           sentence  \\\n",
       "0    harrassment   \"OP, you're a complete moron and an embarrass...   \n",
       "1  No Violations   \"Irrational hate of something they don't unde...   \n",
       "2  No Violations   \"Let me clarify this. I don't believe that an...   \n",
       "3  No Violations   \"DAmnnit, woke up to find this xD I've preord...   \n",
       "4  No Violations   \"I would like to try TERA without waiting so ...   \n",
       "\n",
       "                                     final_sentences  \n",
       "0  [\", op, ,, you, ', re, a, complete, mor, ##on,...  \n",
       "1  [\", irrational, hate, of, something, they, don...  \n",
       "2  [\", let, me, clarify, this, ., i, don, ', t, b...  \n",
       "3  [\", damn, ##ni, ##t, ,, woke, up, to, find, th...  \n",
       "4  [\", i, would, like, to, try, ter, ##a, without...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_sentences']=final_sentences  \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96d3fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "      <th>context</th>\n",
       "      <th>is_violation</th>\n",
       "      <th>violation</th>\n",
       "      <th>sentence</th>\n",
       "      <th>final_sentences</th>\n",
       "      <th>final_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>harrassment</td>\n",
       "      <td>\"OP, you're a complete moron and an embarrass...</td>\n",
       "      <td>[\", op, ,, you, ', re, a, complete, mor, ##on,...</td>\n",
       "      <td>[[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Irrational hate of something they don't unde...</td>\n",
       "      <td>[\", irrational, hate, of, something, they, don...</td>\n",
       "      <td>[[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qdole</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2012-03-01 23:59:45</td>\n",
       "      <td>['[\"The hypocrisy is interesting to watch, tha...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"Let me clarify this. I don't believe that an...</td>\n",
       "      <td>[\", let, me, clarify, this, ., i, don, ', t, b...</td>\n",
       "      <td>[[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"DAmnnit, woke up to find this xD I've preord...</td>\n",
       "      <td>[\", damn, ##ni, ##t, ,, woke, up, to, find, th...</td>\n",
       "      <td>[[, ', [, \", well, i, would, say, i, want, one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qdoi3</td>\n",
       "      <td>TeraOnline</td>\n",
       "      <td>2012-03-01 23:57:45</td>\n",
       "      <td>['[\"well i would say i want one because the ga...</td>\n",
       "      <td>No</td>\n",
       "      <td>No Violations</td>\n",
       "      <td>\"I would like to try TERA without waiting so ...</td>\n",
       "      <td>[\", i, would, like, to, try, ter, ##a, without...</td>\n",
       "      <td>[[, ', [, \", well, i, would, say, i, want, one...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     subreddit         created_date  \\\n",
       "0  qdole  Conservative  2012-03-01 23:59:45   \n",
       "1  qdole  Conservative  2012-03-01 23:59:45   \n",
       "2  qdole  Conservative  2012-03-01 23:59:45   \n",
       "3  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "4  qdoi3    TeraOnline  2012-03-01 23:57:45   \n",
       "\n",
       "                                             context is_violation  \\\n",
       "0  ['[\"The hypocrisy is interesting to watch, tha...          Yes   \n",
       "1  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "2  ['[\"The hypocrisy is interesting to watch, tha...           No   \n",
       "3  ['[\"well i would say i want one because the ga...           No   \n",
       "4  ['[\"well i would say i want one because the ga...           No   \n",
       "\n",
       "       violation                                           sentence  \\\n",
       "0    harrassment   \"OP, you're a complete moron and an embarrass...   \n",
       "1  No Violations   \"Irrational hate of something they don't unde...   \n",
       "2  No Violations   \"Let me clarify this. I don't believe that an...   \n",
       "3  No Violations   \"DAmnnit, woke up to find this xD I've preord...   \n",
       "4  No Violations   \"I would like to try TERA without waiting so ...   \n",
       "\n",
       "                                     final_sentences  \\\n",
       "0  [\", op, ,, you, ', re, a, complete, mor, ##on,...   \n",
       "1  [\", irrational, hate, of, something, they, don...   \n",
       "2  [\", let, me, clarify, this, ., i, don, ', t, b...   \n",
       "3  [\", damn, ##ni, ##t, ,, woke, up, to, find, th...   \n",
       "4  [\", i, would, like, to, try, ter, ##a, without...   \n",
       "\n",
       "                                       final_context  \n",
       "0  [[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...  \n",
       "1  [[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...  \n",
       "2  [[, ', [, \", the, h, ##yp, ##oc, ##ris, ##y, i...  \n",
       "3  [[, ', [, \", well, i, would, say, i, want, one...  \n",
       "4  [[, ', [, \", well, i, would, say, i, want, one...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context=df['context']\n",
    "final_context=[]\n",
    "for i in context:\n",
    "    final_context.append(reddit_clean(i))\n",
    "df['final_context']=final_context  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c316fa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45762"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"label\"]=encoder.fit_transform(df[\"violation\"])\n",
    "#df['final_sentences']=df['final_context']+df['final_sentences']\n",
    "\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(np.array(df['sentence']).flatten().tolist(), df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    train_size=0.75, \n",
    "                                                                    stratify=df['label'])\n",
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0bff00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "   list(train_text),\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "   list(val_text),\n",
    "    max_length = 512,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd42f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582d7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y = torch.tensor(list(val_labels))\n",
    "train_y = torch.tensor(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687ce344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\",return_dict=False)\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867bb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert,label_map):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,len(label_map))\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e5cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map=le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "model = BERT_Arch(bert,label_map)\n",
    "\n",
    "# push the model to GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d591005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\py22715\\.conda\\envs\\v38\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8120770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    total_labels =[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in tenumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 5000 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad()        \n",
    "        preds = model(sent_id,mask)\n",
    "        loss = loss_func(preds, labels)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.tolist()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    f1 = f1_score(total_labels, total_preds, average='micro')\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f15c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step,batch in tenumerate(val_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 5000 == 0 and not step == 0:\n",
    "\n",
    "          # Calculate elapsed time in minutes.\n",
    "          #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "          # Report progress.\n",
    "          print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_func(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            total_preds+=list(preds)\n",
    "            total_labels+=labels.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    f1 = f1_score(total_labels, total_preds, average='micro')\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18453f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45762\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7222463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5004/45762 [03:01<25:21, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10005/45762 [06:10<22:37, 26.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15003/45762 [09:20<19:30, 26.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 20004/45762 [12:30<16:19, 26.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 25005/45762 [15:40<13:08, 26.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 30004/45762 [18:51<10:03, 26.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 35005/45762 [22:01<06:52, 26.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 40003/45762 [25:12<03:38, 26.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 45004/45762 [28:23<00:28, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 45,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45762/45762 [28:51<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.6393065083651936\n",
      "0.5574494121760413\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5004/15254 [02:54<05:57, 28.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 10005/15254 [05:49<03:03, 28.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 15006/15254 [08:44<00:08, 28.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15254/15254 [08:52<00:00, 28.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.6393065083651936\n",
      "0.5574494121760413\n",
      "\n",
      "Training Loss: 1.639\n",
      "Validation Loss: 1.321\n",
      "\n",
      "Training F1: 0.557\n",
      "Validation F1: 0.581\n",
      "\n",
      " Epoch 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5004/45762 [03:10<25:50, 26.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10002/45762 [06:21<22:46, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15003/45762 [09:32<19:32, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 20004/45762 [12:42<16:24, 26.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 25005/45762 [15:53<13:15, 26.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 30003/45762 [19:03<09:58, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 35004/45762 [22:14<06:50, 26.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 40002/45762 [25:24<03:38, 26.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 45003/45762 [28:35<00:28, 26.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 45,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45762/45762 [29:04<00:00, 26.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2640384711643329\n",
      "0.611249508325685\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5003/15254 [02:54<05:56, 28.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 10004/15254 [05:49<03:04, 28.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 15005/15254 [08:44<00:08, 28.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15254/15254 [08:53<00:00, 28.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2640384711643329\n",
      "0.611249508325685\n",
      "\n",
      "Training Loss: 1.264\n",
      "Validation Loss: 1.152\n",
      "\n",
      "Training F1: 0.611\n",
      "Validation F1: 0.644\n",
      "\n",
      " Epoch 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5004/45762 [03:10<25:53, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10005/45762 [06:21<22:44, 26.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15003/45762 [09:32<19:23, 26.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 20004/45762 [12:42<16:23, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 25005/45762 [15:53<13:11, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 30003/45762 [19:03<10:05, 26.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 35004/45762 [22:13<06:49, 26.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 40005/45762 [25:24<03:38, 26.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 45003/45762 [28:34<00:28, 26.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 45,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45762/45762 [29:03<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2425195611602302\n",
      "0.6408592281805865\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5003/15254 [02:54<05:59, 28.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 10004/15254 [05:49<03:03, 28.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 15005/15254 [08:44<00:08, 28.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15254/15254 [08:53<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2425195611602302\n",
      "0.6408592281805865\n",
      "\n",
      "Training Loss: 1.243\n",
      "Validation Loss: 1.212\n",
      "\n",
      "Training F1: 0.641\n",
      "Validation F1: 0.655\n",
      "\n",
      " Epoch 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5004/45762 [03:10<25:41, 26.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10005/45762 [06:21<22:48, 26.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15003/45762 [09:31<19:25, 26.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 20004/45762 [12:42<16:13, 26.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 25002/45762 [15:52<13:11, 26.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 30003/45762 [19:03<10:03, 26.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 35004/45762 [22:13<06:49, 26.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 40005/45762 [25:23<03:38, 26.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 45003/45762 [28:34<00:29, 26.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 45,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45762/45762 [29:03<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2713956227623708\n",
      "0.648616756260653\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5004/15254 [02:56<06:06, 28.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 10004/15254 [05:51<03:04, 28.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 15005/15254 [08:46<00:08, 28.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15254/15254 [08:55<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2713956227623708\n",
      "0.648616756260653\n",
      "\n",
      "Training Loss: 1.271\n",
      "Validation Loss: 1.236\n",
      "\n",
      "Training F1: 0.649\n",
      "Validation F1: 0.661\n",
      "\n",
      " Epoch 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5004/45762 [03:10<25:47, 26.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10005/45762 [06:21<22:46, 26.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15003/45762 [09:31<19:26, 26.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 20004/45762 [12:41<16:25, 26.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 25005/45762 [15:52<13:06, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 30003/45762 [19:02<09:55, 26.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 35004/45762 [22:13<06:51, 26.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 40005/45762 [25:23<03:39, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 45003/45762 [28:33<00:29, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 45,000  of  45,762.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45762/45762 [29:02<00:00, 26.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2755169984635695\n",
      "0.6538831344783882\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5003/15254 [02:54<05:57, 28.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 10004/15254 [05:49<03:04, 28.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 15005/15254 [08:44<00:08, 28.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,000  of  15,254.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15254/15254 [08:53<00:00, 28.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and F1\n",
      "1.2755169984635695\n",
      "0.6538831344783882\n",
      "\n",
      "Training Loss: 1.276\n",
      "Validation Loss: 1.228\n",
      "\n",
      "Training F1: 0.654\n",
      "Validation F1: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, f1_train = train()\n",
    "    print(\"Training loss and F1\")\n",
    "    print(train_loss)\n",
    "    print(f1_train)\n",
    "    #evaluate model\n",
    "    valid_loss, f1_valid = evaluate()\n",
    "    print(\"Training loss and F1\")\n",
    "    print(train_loss)\n",
    "    print(f1_train)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        file_name = 'topic_saved_weights.pt'\n",
    "        #save_checkpoint(file_name, epoch, model, optimizer, label_map, id2label)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "    print(f'\\nTraining F1: {f1_train:.3f}')\n",
    "    print(f'Validation F1: {f1_valid:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c9866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
